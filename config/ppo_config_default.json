

 
/*PPO related configuration.
Defaults
//"use_critic": True 
//self.use_gae: True
//self.lambda_: 1.0
//self.sgd_minibatch_size = 128
//self.num_sgd_iter = 30
//self.shuffle_sequences = True
//self.vf_loss_coeff = 1.0
//self.entropy_coeff = 0.0
//self.entropy_coeff_schedule = None
//self.clip_param = 0.3
//self.vf_clip_param = 10.0
//self.grad_clip = None
//self.model["vf_share_layers"] = False
//self._disable_preprocessor_api = False

 Args:
     lr_schedule: Learning rate schedule. In the format of
         [[timestep, lr-value], [timestep, lr-value], ...]
         Intermediary timesteps will be assigned to interpolated learning rate
         values. A schedule should normally start from timestep 0.
     use_critic: Should use a critic as a baseline (otherwise don't use value
         baseline; required for using GAE).
     use_gae: If true, use the Generalized Advantage Estimator (GAE)
         with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
     lambda_: The GAE (lambda) parameter.
     kl_coeff: Initial coefficient for KL divergence.
     sgd_minibatch_size: Total SGD batch size across all devices for SGD.
         This defines the minibatch size within each epoch.
     num_sgd_iter: Number of SGD iterations in each outer loop (i.e., number of
         epochs to execute per train batch).
     shuffle_sequences: Whether to shuffle sequences in the batch when training
         (recommended).
     vf_loss_coeff: Coefficient of the value function loss. IMPORTANT: you must
         tune this if you set vf_share_layers=True inside your model's config.
     entropy_coeff: Coefficient of the entropy regularizer.
     entropy_coeff_schedule: Decay schedule for the entropy regularizer.
     clip_param: PPO clip parameter.
     vf_clip_param: Clip param for the value function. Note that this is
         sensitive to the scale of the rewards. If your expected V is large,
         increase this.
     grad_clip: If specified, clip the global norm of gradients by this amount.
     kl_target: Target value for KL divergence.

 Returns:
     This updated AlgorithmConfig object.
     */